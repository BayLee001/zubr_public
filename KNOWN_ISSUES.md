Known Issues
============

This document describes known issues in Zubr. 

mod.main
----------------------------------

for now, the pipeline can only call mod.main imports, should be changes
so that script files can call arbitrary functions 

lisp defun
------------------------------------
Lisp defun has issues when the comment spans multiple lines

gzip OverflowError
-------------------------------------

ZubrClass automatically gzips large objects (e.g., models), and
crashes when the models get large, resulting in an
OverflowError. Apparently this is a underlying problem in python 2.7 

http://stackoverflow.com/questions/33562394/gzip-raised-overflowerror-size-does-not-fit-in-an-unsigned-int

I've tried to replace gzip with bz2, which is very slow. Current I'm
using lz4 compression for large models (dump_large is the name of the
ZubrClass method used). 

concurrent graph decoder
---------------------------------------
The concurrent graph decoders sometimes ranks slightly different than
the non-concurrent models. Perhaps there is a slight difference in the
models used when copied, or when uses a trained model from file. The
difference so far doesn't seem significant, but might make a
difference in the evaluation if comparing output generated by the
different decoders.

executable prolog models and decoders
----------------------------------------
The executable models for querying prolog databases (e.g., in the
Geoquery domain) have all sorts of annoying issues... despite my best
efforts, they don't always exit swipl prolog after a failure. Also,
the subprocess module tends to fail when multiple instances of prolog
are being called, meaning that it's best to run experiments
individually.

When running an executable semantic parser, the prolog crashes when
run along side multiple instances, so you should read each model
separately. 

numpy/cython version differences?
----------------------------------------
The library was largely developed using Python 2.7, numpy 1.13.1 and
cython 0.23.4; I noticed some weird errors and slightly varied
experimental results (nothing signficiant, as far as I can tell) when
the servers were updated to Cython 0.28.5 and numpy 1.15.1. Be mindful
of this is something doesn't work.

Datasets with constant length
---------------------------------------
Datasets are treated as 2d np array objects with np.object dtypes,
which allows for variable row lengths (i.e., different input
lengths). However, when the length is constant across all examples, numpy automatically
makes the internal dimension immutable, and throws a ValueError if you
try to modify the entries to representation with a different dimension
(this happens in zubr/neural/util when we take the original data
representations and pad them with EOS labels, which increases the
dimension by 1). This also somehow screws up the Alignment classes,
since it appears to be using the .shape attribute without expecting a
second dimension (this is speculatio)

Neural Graph Decoders
-------------------------------------------
The neural graph decoders (and the neural machinery more generally) does 
not scale well with vocabulary size; in particular, training the code
models in the NAACL paper and doing decoding takes ages. There are many
optimizations possible here that need to be implemented, just keep in mind
that the neural models were primarily developed for small semantic parisng datasets
of the geoquery variety.   